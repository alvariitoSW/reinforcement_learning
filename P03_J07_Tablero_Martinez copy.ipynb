{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njb_ProuHiOe"
   },
   "source": [
    "## Lab Assignment: Reinforcement learning\n",
    "\n",
    "#### Lab team: J07\n",
    "\n",
    "##### Name (member 1): H√©ctor Tablero D√≠az\n",
    "\n",
    "##### Name (member 2): √Ålvaro Mart√≠nez Gamo\n",
    "\n",
    "* Please include your full name at the beginning of all submitted files.\n",
    "* Make sure the presentation is well-structured: the report will be evaluated not only for correctness, but also for clarity, conciseness, and completeness.\n",
    "* Make use of figures and tables to summarize the results and illustrate the discussions.\n",
    "* If external material is used, the sources must be cited. \n",
    "* Include references in APA format https://pitt.libguides.com/citationhelp/apa7. Lack or poorly formatted references can be penalized.\n",
    "* A generative AI tool can be used for consultation. You must specify the tool used in your report.\n",
    "* You are not allowed to use a generative AI tool to generate code.\n",
    "\n",
    "Submit a single `.zip` file, whose name has the format `AA3_2024_2025_P03_teamCode_lastName1_lastName2.zip` \n",
    "The name must not include graphical accents, spaces, uppercase letters, or special characters.\n",
    "\n",
    "For example: `AA3_2024_2025_P03_V03_munyoz_deLaRosa.zip`\n",
    "\n",
    "This compressed file must include the following files:\n",
    "\n",
    "* This Python notebook with the solutions of the exercises. The notebook should include only code snippets, figures, tables, derviations and explanations (with LaTex if necessary) in Markdown cells. Handwritten material can be included in the Python notebook as images. Functions should be defined in a separate `.py` file, not in the notebook.\n",
    "* The necessary `.py` and addional files to ensure the Python notebook code can be executed sequentially without errors.\n",
    "* A PDF file generated from the notebook (Export the notebook as an HTML file. Open the HTML file in a Browser and print it as a PDF file). \n",
    "\n",
    "Make sure that all the code cells can be executed squentially without errors (Kernel -> Restart & Run All). Exectution and formatting errors will be penalized.\n",
    "\n",
    "The grade of this lab assignment is based on\n",
    "* This submission (50 %).\n",
    "* An individual in-class exam (50%).\n",
    "\n",
    "\n",
    "Evaluation criteria:\n",
    "* [6 points] Quality of the report (correctness, clarity, conciseness, completeness).\n",
    "* [3 points] Quality of the code (correctness, adherence to a Python style guide -for instance, Google's-, comments, functional decomposition).\n",
    "* [1 point]  References.                                                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEtx8Y8MqKfH"
   },
   "source": [
    "# Training a reinforcemnt learning agent at the gymnasium üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö RL-environment:\n",
    "\n",
    "- Python and NumPy\n",
    "- [Gymnasium](https://gymnasium.farama.org/)\n",
    "\n",
    "### üéÆ Environment:\n",
    "\n",
    "- [FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "\n",
    "\n",
    "### üìö Pygame:\n",
    "\n",
    "https://www.pygame.org/wiki/about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: The gymnasium\n",
    "\n",
    "#### TO DO:  Complete the gymnasium tutorials\n",
    "* Basic usage: https://gymnasium.farama.org/introduction/basic_usage/\n",
    "* Training an RL-agent: https://gymnasium.farama.org/introduction/train_agent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcNvOAQlysBJ"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import rl_utils\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "from reinforcement_learning import (\n",
    "    sarsa_learning,\n",
    "    q_learning, \n",
    "    greedy_policy,\n",
    "    epsilon_greedy_policy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages if needed\n",
    "# %pip install pygame\n",
    "# %pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp4-bXKIy1mQ"
   },
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Q-learning\n",
    "\n",
    "We're now ready to code our Q-Learning algorithm üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xya49aNJWVvv"
   },
   "source": [
    "## Step 0: Set up and understand Frozen Lake environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaW_LHfS0PY2"
   },
   "source": [
    "Let's begin with a simple 4x4 map and non-slippery, meaning the agent always moves in the desired direction.\n",
    "\n",
    "We add a parameter called `render_mode` that specifies how the environment should be visualised. In our case because we **want to record a video of the environment at the end, we need to set render_mode to rgb_array**.\n",
    "\n",
    "As [explained in the documentation](https://gymnasium.farama.org/api/env/#gymnasium.Env.render) ‚Äúrgb_array‚Äù: Return a single frame representing the current state of the environment. \n",
    "A frame is a np.ndarray with shape (H, W, 3) representing RGB values for an H (height) times W (width) pixel image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNxUbPMP0akP"
   },
   "outputs": [],
   "source": [
    "small_environment = gym.make(\n",
    "    'FrozenLake-v1', \n",
    "    map_name='4x4', \n",
    "    is_slippery=False, \n",
    "    render_mode='rgb_array',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXbTfdeJ1Xi9"
   },
   "source": [
    "### Let's see what the environment looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = small_environment.reset()  # observation state\n",
    "action_names = {0: 'Left', 1: 'Down', 2: 'Right', 3: 'Up'}\n",
    "\n",
    "print(state)\n",
    "print(info, '(Probability that the action has led to the current state)')\n",
    "n_states = small_environment.observation_space.n\n",
    "print(\"There are \", n_states, \" possible states\")\n",
    "\n",
    "n_actions = small_environment.action_space.n\n",
    "print(\"There are \", n_actions, \" possible actions\")\n",
    "print(action_names)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "game_image = ax.imshow(small_environment.render())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNPG0g_UGCfh"
   },
   "outputs": [],
   "source": [
    "# Generate a random observed state\n",
    "print(\"Observation state (randomly selected)\", small_environment.observation_space.sample()) \n",
    "\n",
    "# Generate a random action from the current state\n",
    "print(\"Action (randomly selected):\", small_environment.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Generate an episode\n",
    "\n",
    "obsevation, info = small_environment.reset()  # state state_state\n",
    "fig, ax = plt.subplots()\n",
    "game_image = ax.imshow(small_environment.render())\n",
    "\n",
    "\n",
    "MAX_STEPS = 100\n",
    "refresh_rate = 1 # in (1 / seconds)\n",
    "\n",
    "episode_over = False\n",
    "n_steps = 0\n",
    "   \n",
    "while not episode_over and n_steps < MAX_STEPS:\n",
    "    n_steps += 1\n",
    "    action = small_environment.action_space.sample()  \n",
    "\n",
    "    state, reward, terminated, truncated, info = small_environment.step(action)\n",
    "    episode_over = terminated or truncated\n",
    "    \n",
    "    ax.set_title(\n",
    "        'Step: {}  State: {}  Reward: {} Action:{}'.format(\n",
    "            n_steps, \n",
    "            state,\n",
    "            reward,\n",
    "            action_names[action],\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    display(fig)\n",
    "    time.sleep(1.0 / refresh_rate)\n",
    "    clear_output(wait=True)  # Clear previous output\n",
    "    game_image.set_data(small_environment.render())\n",
    "\n",
    "small_environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Atll4Z774gri"
   },
   "source": [
    "## Step 1: Greedy and Epsilon greedy policies\n",
    "\n",
    "Since Q-Learning is an **off-policy** algorithm, we have two policies. This means we're using a **different policy for acting and updating the value function**.\n",
    "\n",
    "- Epsilon-greedy policy (acting policy)\n",
    "- Greedy-policy (updating policy)\n",
    "\n",
    "The greedy policy will also be the final policy we'll have when the Q-learning agent completes training. The greedy policy is used to select an action using the Q-table.\n",
    "\n",
    "Epsilon-greedy is the training policy that handles the exploration/exploitation trade-off.\n",
    "\n",
    "- With *probability 1‚Ää-‚Ää…õ* : **we do exploitation** (i.e. our agent selects the action with the highest state-action pair value).\n",
    "\n",
    "- With *probability …õ*: we do **exploration** (trying a random action).\n",
    "\n",
    "As the training continues, we progressively **reduce the epsilon value since we will need less and less exploration and more exploitation.**\n",
    "\n",
    "#### TO DO: Implement these policies in `reinforcement_learning.py`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDb7Tdx8atfL"
   },
   "source": [
    "# Step 2: Train the RL agent üèÉ\n",
    "\n",
    "#### TO DO: Implement the Q-learning algorithm in `reinforcement_learning.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hW80DealcRtu"
   },
   "source": [
    "## Define the hyperparameters for the learning process ‚öôÔ∏è\n",
    "\n",
    "The exploration related hyperparamters are some of the most important ones.\n",
    "\n",
    "- We need to make sure that our agent **explores enough of the state space** to learn a good value approximation. To do that, we need to have progressive decay of the epsilon.\n",
    "- If you decrease epsilon too fast (too high decay_rate), **you take the risk that your agent will be stuck** in a local optimum, since your agent didn't explore enough of the state space and hence can't solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1tWn0tycWZ1"
   },
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "\n",
    "n_training_episodes = 1000\n",
    "max_steps = 100 # Maximum number of steps per episode\n",
    "learning_rate = 0.7\n",
    "gamma = 0.95 # Discount factor\n",
    " \n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0  # Initial exploration probability\n",
    "min_epsilon = 0.05  # Minimum exploration probability\n",
    "decay_rate = 0.0005 # Exponential decay rate for the exploration probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPBxfjJdTCOH"
   },
   "outputs": [],
   "source": [
    "# Initialize Q-table\n",
    "Qtable_small = np.zeros(\n",
    "    (\n",
    "        small_environment.observation_space.n, \n",
    "        small_environment.action_space.n\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Learn Q-table\n",
    "Qtable_small = q_learning(\n",
    "    small_environment, \n",
    "    n_training_episodes,\n",
    "    max_steps,\n",
    "    learning_rate,\n",
    "    gamma,\n",
    "    min_epsilon, \n",
    "    max_epsilon, \n",
    "    decay_rate,  \n",
    "    Qtable_small,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVeEhUCrc30L"
   },
   "source": [
    "### Let's see what our Q-Learning table looks like now üëÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmfchsTITw4q"
   },
   "outputs": [],
   "source": [
    "Qtable_small  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...and what our agent is doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BaUf2HFvZRjR"
   },
   "outputs": [],
   "source": [
    "frames = rl_utils.generate_greedy_episode(small_environment, Qtable_small)\n",
    "rl_utils.show_episode(frames, interval=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A more challenging problem\n",
    "\n",
    "We're ready now to find our way in more challenging environments üí•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_environment = gym.make(\n",
    "    'FrozenLake-v1', \n",
    "    map_name='8x8', \n",
    "    is_slippery=False, \n",
    "    render_mode='rgb_array'\n",
    ")\n",
    "\n",
    "n_states = large_environment.observation_space.n\n",
    "print(\"There are \", n_states, \" possible states\")\n",
    "\n",
    "n_actions = large_environment.action_space.n\n",
    "print(\"There are \", n_actions, \" possible actions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Training hyperparameters\n",
    "n_training_episodes = 10000\n",
    "max_steps = 700\n",
    "learning_rate = 0.7\n",
    "gamma = 0.99\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.1\n",
    "decay_rate = 0.00001\n",
    "\n",
    "\n",
    "# Initialize Q-table\n",
    "Qtable_large = np.zeros(\n",
    "    (\n",
    "        large_environment.observation_space.n,\n",
    "        large_environment.action_space.n\n",
    "    )\n",
    ")\n",
    "\n",
    "# Learn Q-table\n",
    "Qtable_large = q_learning(\n",
    "    large_environment, \n",
    "    n_training_episodes,\n",
    "    max_steps,\n",
    "    learning_rate,\n",
    "    gamma,\n",
    "    min_epsilon, \n",
    "    max_epsilon, \n",
    "    decay_rate,  \n",
    "    Qtable_large,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = rl_utils.generate_greedy_episode(large_environment, Qtable_large)\n",
    "rl_utils.show_episode(frames, interval=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slippery environment\n",
    "\n",
    "Our environment is now slippery, meaning the agent sometimes slips and moves in an unintended direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slippery_environment = gym.make(\n",
    "    'FrozenLake-v1', \n",
    "    map_name='8x8', \n",
    "    is_slippery=True, \n",
    "    render_mode='rgb_array',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the challenges in this environment, let's make our agent go right several times and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_right = []\n",
    "action = 2  # go-right\n",
    "n_steps = 20\n",
    "state, info = slippery_environment.reset()\n",
    "for i in range(n_steps):\n",
    "    go_right.append(slippery_environment.render())  # Capture current frame (RGB array)\n",
    "    slippery_environment.step(action)\n",
    "\n",
    "rl_utils.show_episode(go_right, interval=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens when we train our agent in the same way as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Procedemos a volver a cambiar los hiperparametros de tal forma que el agente llegue al objetivo con el atributo de `is_slippery` en **True**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Training hyperparameters\n",
    "####################################################################################\n",
    "n_training_episodes = 100000\n",
    "max_steps = 800\n",
    "learning_rate = 0.1\n",
    "gamma = 0.995\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.5\n",
    "decay_rate = 0.00001\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "# Initialize Q-table\n",
    "Qtable_slippery = np.zeros(\n",
    "    (\n",
    "        slippery_environment.observation_space.n, \n",
    "        slippery_environment.action_space.n\n",
    "    )\n",
    ")\n",
    "\n",
    "# Learn Q-table\n",
    "Qtable_slippery = q_learning(\n",
    "    slippery_environment, \n",
    "    n_training_episodes,\n",
    "    max_steps,\n",
    "    learning_rate,\n",
    "    gamma,\n",
    "    min_epsilon, \n",
    "    max_epsilon, \n",
    "    decay_rate,\n",
    "    Qtable_slippery,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = rl_utils.generate_greedy_episode(slippery_environment, Qtable_slippery)\n",
    "rl_utils.show_episode(frames, interval=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  Podemos obervar como el agente tiene una mayor dificultad de encontrar el objetivo debido a la dificultad a√±adida por el `enviroment`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Train the RL agent using SARSA üèÉ\n",
    "\n",
    "#### TO DO: Implement the SARSA learning algorithm in `reinforcement_learning.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Procedemos a replicar el entrenamiento anterior pero ahora usando el algoritmo `SARSA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_environment = gym.make(\n",
    "    'FrozenLake-v1', \n",
    "    map_name='8x8', \n",
    "    is_slippery=False, \n",
    "    render_mode='rgb_array'\n",
    ")\n",
    "\n",
    "# Hiperpar√°metros optimizados\n",
    "n_training_episodes  = 10000\n",
    "max_steps = 700\n",
    "learning_rate = 0.1\n",
    "gamma = 0.995\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.5\n",
    "decay_rate = 0.0001\n",
    "\n",
    "\n",
    "# Inicializar Q-table\n",
    "Qtable_large_sarsa = np.zeros(\n",
    "    (\n",
    "        large_environment.observation_space.n,\n",
    "        large_environment.action_space.n\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Aprender Q-table\n",
    "Qtable_large_sarsa = sarsa_learning(\n",
    "    large_environment, \n",
    "    n_training_episodes,\n",
    "    learning_rate,\n",
    "    gamma,\n",
    "    min_epsilon, \n",
    "    max_epsilon, \n",
    "    decay_rate,\n",
    "    max_steps,\n",
    "    Qtable_large_sarsa,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = rl_utils.generate_greedy_episode(large_environment, Qtable_large_sarsa)\n",
    "rl_utils.show_episode(frames, interval=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Compare SARSA and Q-learning\n",
    "\n",
    "Answer these questions:\n",
    "* What is \"episode reward\" in RL and how is it related to the performance of an RL agent?\n",
    "* What is \"episode length\" in RL and how is it related to the performance of an RL agent?\n",
    "* What is \"training error\" in RL and how is it related to the performance of an RL agent?\n",
    "    * Provide an explicit expression of the training error in the cases explored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### 1. Recompensa de Episodio en RL (episode reward)\n",
    "**Definici√≥n**:  \n",
    "Suma total de recompensas acumuladas por un agente desde el estado inicial hasta el terminal en un episodio.\n",
    "\n",
    "**F√≥rmula**:  \n",
    "$$  \n",
    "\\text{Recompensa de Episodio} = \\sum_{t=0}^{T} r_t  \n",
    "$$  \n",
    "**Variables**:  \n",
    "- $ r_t $: Recompensa en el paso $ t $.  \n",
    "- $ T $: Total de pasos en el episodio.  \n",
    "\n",
    "**Relaci√≥n con el Rendimiento**:  \n",
    "-  **Valores altos = Mejor rendimiento**: Indica pol√≠ticas exitosas para maximizar recompensas.  \n",
    "-  **Seguimiento de progreso**: Monitoreo de cambios durante el entrenamiento.  \n",
    "-  **Indicador de convergencia**: Estabilizaci√≥n sugiere aprendizaje completo.  \n",
    "-  **Comparaci√≥n de pol√≠ticas**: M√©trica para evaluar algoritmos/configuraciones.\n",
    "\n",
    "\n",
    "### 2. Longitud de Episodio en RL (episode length)\n",
    "**Definici√≥n**:  \n",
    "N√∫mero de pasos realizados por el agente antes de terminar un episodio.\n",
    "\n",
    "**Relaci√≥n con el Rendimiento**:  \n",
    "| Tipo de Entorno                 | Longitud Ideal          | Explicaci√≥n                          |  \n",
    "|----------------------------------|-------------------------|--------------------------------------|  \n",
    "| Entornos con objetivos           | Corta (‚¨áÔ∏è)             | Camino eficiente al objetivo.        |  \n",
    "| Entornos de \"supervivencia\"      | Larga (‚¨ÜÔ∏è)              | Evita terminaci√≥n prematura.         |  \n",
    "| Entornos con penalizaci√≥n/paso   | Corta (‚¨áÔ∏è)             | Minimiza recompensas negativas.      |  \n",
    "\n",
    "**Caso Espec√≠fico (Frozen Lake)**:  \n",
    "- Longitudes cortas = Pol√≠tica eficiente puesto que evita desv√≠os innecesarios.\n",
    "\n",
    "\n",
    "\n",
    "## 3. Error de Entrenamiento en RL (training error)\n",
    "**Definici√≥n**:  \n",
    "Diferencia entre las estimaciones de valor del agente y el valor objetivo (error TD).\n",
    "\n",
    "**F√≥rmulas por Algoritmo**:  \n",
    "- **Q-learning**:  \n",
    "  $$  \n",
    "  \\text{Error TD} = r_t + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)  \n",
    "  $$  \n",
    "- **SARSA**:  \n",
    "  $$  \n",
    "  \\text{Error TD} = r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)  \n",
    "  $$  \n",
    "\n",
    "**Variables**:  \n",
    "- $ \\gamma $: Factor de descuento.  \n",
    "- $ Q(s_t, a_t) $: Valor estimado de la acci√≥n $ a_t $ en estado $ s_t $.  \n",
    "\n",
    "**Error Cuadr√°tico Medio (MSE)**:  \n",
    "$$  \n",
    "\\text{MSE} = \\frac{1}{T} \\sum_{t=0}^{T-1} (\\text{Error TD}_t)^2  \n",
    "$$  \n",
    "\n",
    "**Relaci√≥n con el Rendimiento**:  \n",
    "-  **Disminuci√≥n del error**: Indica convergencia del aprendizaje.  \n",
    "-  **Estabilidad**: Error estable = Pol√≠tica consistente.  \n",
    "-  **Progreso**: Tasa de reducci√≥n refleja velocidad de aprendizaje.  \n",
    "-  **Comparaci√≥n algor√≠tmica**: Diferencias en error revelan estabilidad din√°mica.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Compare SARSA and Q-learning\n",
    "\n",
    "* Use matplotlib to compare the learning curves of SARSA and Q-learning, in terms of \n",
    "    * episode reward. \n",
    "    * episode length.\n",
    "    * training error\n",
    "* Discuss the results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ahora procedemos a comparar la curva de ambos algoritmos en los terminos explicados en el ejercicio anterior y por facilidad de c√≥mputo y posterior discusi√≥n de los resultados, se va a usar el environment con el atributo `is_slippery=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_utils import run_experiments\n",
    "\n",
    "large_environment = gym.make(\n",
    "    'FrozenLake-v1',\n",
    "    map_name='8x8',\n",
    "    is_slippery=False,\n",
    "    render_mode='rgb_array'\n",
    ")\n",
    "\n",
    "q_learning_param = {\n",
    "    'n_training_episodes': 10000,\n",
    "    'max_steps': 700,\n",
    "    'learning_rate': 0.7,\n",
    "    'gamma': 0.99,\n",
    "    'max_epsilon': 1.0,\n",
    "    'min_epsilon': 0.1,\n",
    "    'decay_rate': 0.0001,\n",
    "    'Q_table': Qtable_large # tabla obtenida en celdas anteriores\n",
    "}\n",
    "\n",
    "sarsa_param = {\n",
    "    'n_training_episodes': 10000,\n",
    "    'max_steps': 700,\n",
    "    'learning_rate': 0.1,\n",
    "    'gamma': 0.995,\n",
    "    'max_epsilon': 1.0,\n",
    "    'min_epsilon': 0.5,\n",
    "    'decay_rate': 0.0001,\n",
    "    'Q_table_sarsa' : Qtable_large_sarsa  # tabla obtenida en celdas anteriores\n",
    "}\n",
    "\n",
    "run_experiments(large_environment, q_learning_param, sarsa_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discusi√≥n sobre las diferencias entre **SARSA** y **Q-learning**\n",
    "\n",
    "\n",
    "### - An√°lisis de la Recompensa Media:\n",
    "\n",
    "En la primera gr√°fica, observamos que **SARSA** (üî¥) logra consistentemente **recompensas m√°s altas** que **Q-learning** (üîµ), especialmente despu√©s de los **4000 episodios**. Esta diferencia se hace m√°s pronunciada hacia el final del entrenamiento.\n",
    "\n",
    "> **Conclusi√≥n r√°pida:** SARSA consigue mejores resultados en entornos donde el riesgo debe ser gestionado cuidadosamente.\n",
    "\n",
    "Esta diferencia se debe fundamentalmente a la naturaleza **on-policy** de SARSA frente a la **off-policy** de Q-learning:\n",
    "\n",
    "- üîπ **SARSA (on-policy)**:  \n",
    "  Aprende la pol√≠tica que sigue durante el entrenamiento, **considerando el comportamiento exploratorio**, volvi√©ndose m√°s **conservador** y evitando riesgos innecesarios.\n",
    "\n",
    "- üîπ **Q-learning (off-policy)**:  \n",
    "  Aprende la pol√≠tica √≥ptima **independientemente** de la pol√≠tica seguida durante la exploraci√≥n, apostando por **rutas m√°s arriesgadas** que pueden ser menos estables.\n",
    "\n",
    "> En este entorno espec√≠fico (_FrozenLake_), **SARSA** parece encontrar un mejor equilibrio entre **exploraci√≥n** y **explotaci√≥n**.\n",
    "\n",
    "\n",
    "\n",
    "### - An√°lisis de la Longitud Media de Episodio:\n",
    "\n",
    "La segunda gr√°fica muestra una clara divergencia:\n",
    "\n",
    "- **Q-learning (üîµ)** tiende a episodios **m√°s cortos** (aproximadamente **15 pasos**).\n",
    "- **SARSA (üî¥)** mantiene episodios **m√°s largos** (alrededor de **35-40 pasos**).\n",
    "\n",
    "#### Interpretaci√≥n:\n",
    "\n",
    "1. **Q-learning**:\n",
    "   - Encuentra rutas m√°s **directas** hacia la meta.\n",
    "   - Prioriza caminos **r√°pidos** pero **arriesgados**.\n",
    "\n",
    "2. **SARSA**:\n",
    "   - Prefiere **rutas conservadoras** y **m√°s largas**.\n",
    "   - Evita **estados peligrosos** como los agujeros en _FrozenLake_.\n",
    "\n",
    "\n",
    "> En general Q-learning encuentra caminos m√°s cortos pero peligrosos, por el contrario, SARSA prefiere trayectorias m√°s largas pero seguras, maximizando el √©xito.\n",
    "\n",
    "\n",
    "### - An√°lisis del Error de Entrenamiento Medio:\n",
    "\n",
    "La tercera gr√°fica muestra una diferencia importante:\n",
    "\n",
    "- **SARSA (üî¥)** presenta **errores TD** significativamente **mayores y crecientes**.\n",
    "- **Q-learning (üîµ)** mantiene errores **cercanos a cero**.\n",
    "\n",
    "#### Explicaci√≥n:\n",
    "\n",
    "1. **SARSA**:\n",
    "   - Conciliar **exploraci√≥n** y **explotaci√≥n** genera **inconsistencias** entre las predicciones y los resultados reales.\n",
    "   - El error es alto porque **se sigue adaptando** constantemente.\n",
    "\n",
    "2. **Q-learning**:\n",
    "   - Optimiza directamente hacia la pol√≠tica **√≥ptima te√≥rica**.\n",
    "   - **Converge** m√°s r√°pidamente a una pol√≠tica **estable**, aunque pueda ser **sub√≥ptima globalmente**.\n",
    "\n",
    "> **Importante:** Un mayor error en SARSA no implica peor rendimiento; al contrario, refleja su capacidad de **adaptarse** din√°micamente.\n",
    "\n",
    "\n",
    "\n",
    "## Conclusi√≥n General:\n",
    "\n",
    "Estas gr√°ficas ilustran c√≥mo la **naturaleza on-policy** de **SARSA** frente a la **off-policy** de **Q-learning** afecta a el **comportamiento** de aprendizaje, el **ritmo de convergencia** y el **rendimiento final** en entornos complejos como _FrozenLake_.\n",
    "\n",
    "> **Como hemos visto en las slides de clase, esta gr√°fica ilustra que:**  \n",
    "> **SARSA** apuesta por seguridad y adaptabilidad.  \n",
    "> **Q-learning** apuesta por rapidez y optimalidad te√≥rica.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6: Train a deep Q-learning agent (optional: extra point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Por motivos computacionales y de tiempo, hemos procedido a crear un agente en un environment 4x4 y con una baja optimizaci√≥n de todos los procesos con el objetivo de ver resultados en poco tiempo a pesar de no ser lo mas √≥ptimo con el fin de realizar una brebe exploraci√≥n a cerca del `deep Q-learning`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_agent import ultra_optimized_main\n",
    "\n",
    "ultra_optimized_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Se han dejado las salidas a pesar de no renderizar la barra de progreso puesto que contiene informaci√≥n valiosa del proceso de entrenamiento del modelo.**\n",
    "\n",
    "> A continuaci√≥n se muestra el resultado del entrenamiento del agente.\n",
    "\n",
    "![Promedio de las recompensas del agente deep Q-learning](dqn_training_ultra.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pesar de todas las limitaciones con las que ha operado el agente, ha conseguido obtener ciertos resultados satisfactorios en poco tiempo. <br>\n",
    "Esto nos hace ver que si aumentamos la capacidad de c√≥mputo y buscamos una mayor optimizaci√≥n de parametros e hiperparametros, podemos llegar a conseguir unos resultados altamente precisos."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "67OdoKL63eDD",
    "B2_-8b8z5k54",
    "8R5ej1fS4P2V",
    "Pnpk2ePoem3r"
   ],
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
